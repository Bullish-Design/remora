# Implementation Guide: `xgrammar-tools`

This guide provides the exact implementation for building the new `xgrammar-tools` repository from scratch. Each step includes runnable code representing the final state of that component.

---

## Prerequisite: Repository Initialization

1. Create a new git repository: `xgrammar-tools`.
2. Initialize using `uv`: `uv init xgrammar-tools`
3. Add dependencies: `uv add pydantic httpx openai "xgrammar>=0.1.7" pyyaml`
4. Add dev dependencies: `uv add --dev pytest pytest-asyncio respx`

---

## Step 1: Core Data Models and Configuration

**File: `src/xgrammar_tools/models.py`**
```python
import uuid
from dataclasses import dataclass, field
from typing import Any, Protocol

from pydantic import BaseModel

class KernelConfig(BaseModel):
    """Configuration for the xgrammar-tools execution engine."""
    base_url: str
    model: str
    temperature: float = 0.0
    max_tokens: int = 4096
    timeout_s: float = 60.0

@dataclass(frozen=True)
class ToolCall:
    """A parsed tool call generated by the model."""
    name: str
    arguments: dict[str, Any]
    call_id: str = field(default_factory=lambda: uuid.uuid4().hex[:8])

@dataclass(frozen=True)
class ToolResult:
    """The result of executing a tool."""
    name: str
    output: Any
    call_id: str
    is_error: bool = False

@dataclass(frozen=True)
class KernelResult:
    """The final result returned by the ToolKernel."""
    text: str
    tool_calls: list[ToolCall] = field(default_factory=list)
    tool_results: list[ToolResult] = field(default_factory=list)

class EventCallback(Protocol):
    """Async observer for granular execution events."""
    async def on_model_request(self, payload: dict[str, Any]) -> None: ...
    async def on_model_response(self, payload: dict[str, Any]) -> None: ...
    async def on_tool_execute(self, payload: dict[str, Any]) -> None: ...
    async def on_tool_result(self, payload: dict[str, Any]) -> None: ...
```

**Testing: `tests/test_models.py`**
```python
from xgrammar_tools.models import KernelConfig, ToolCall

def test_config_defaults():
    config = KernelConfig(base_url="http://localhost", model="test")
    assert config.temperature == 0.0

def test_tool_call_auto_id():
    call = ToolCall(name="foo", arguments={})
    assert len(call.call_id) == 8
```

---

## Step 2: Model Plugin Interfaces

**File: `src/xgrammar_tools/plugins/base.py`**
```python
from typing import Any, Protocol
from xgrammar_tools.models import ToolCall

class ModelPlugin(Protocol):
    """Handles model-specific quirks for formatting and parsing."""
    name: str
    
    def build_messages(
        self, prompt: str, system: str | None, context: list[dict[str, Any]] | None
    ) -> list[dict[str, Any]]: ...

    def tool_choice(self, tools: list[dict[str, Any]]) -> Any: ...

    def grammar_strategy(self) -> str | None: ...

    def parse_response(self, response_text: str) -> list[ToolCall]: ...
```

**File: `src/xgrammar_tools/plugins/qwen.py`**
```python
import json
from typing import Any
from xgrammar_tools.models import ToolCall
from xgrammar_tools.plugins.base import ModelPlugin

class QwenPlugin:
    name = "qwen2.5-instruct"

    def build_messages(
        self, prompt: str, system: str | None, context: list[dict[str, Any]] | None
    ) -> list[dict[str, Any]]:
        msgs = []
        if system:
            msgs.append({"role": "system", "content": system})
        if context:
            msgs.extend(context)
        msgs.append({"role": "user", "content": prompt})
        return msgs

    def tool_choice(self, tools: list[dict[str, Any]]) -> Any:
        return "auto" if tools else "none"

    def grammar_strategy(self) -> str | None:
        return "strict_json"

    def parse_response(self, response_text: str) -> list[ToolCall]:
        # Qwen often replies with standard structured json if instructed properly
        calls = []
        if "<tool_call>" in response_text: # simplified regex extraction fallback
             pass # real regex here
        try:
            # Assumes the generation output strictly enforced JSON for the tool
            data = json.loads(response_text)
            if isinstance(data, list):
                for call in data:
                    calls.append(ToolCall(name=call["name"], arguments=call.get("arguments", {})))
        except json.JSONDecodeError:
            pass # Return empty on decode failure, or raise a ParseError
        return calls
```

---

## Step 3: Tool Execution Backend Protocol

**File: `src/xgrammar_tools/backends/base.py`**
```python
from typing import Any, Protocol
from xgrammar_tools.models import ToolCall, ToolResult

class ToolBackend(Protocol):
    async def execute(self, tool_call: ToolCall, inputs: dict[str, Any]) -> ToolResult: ...
```

**File: `src/xgrammar_tools/backends/pym_executor.py`**
```python
import asyncio
import json
import os
from pathlib import Path
from typing import Any
from xgrammar_tools.models import ToolCall, ToolResult

class PymExecutor:
    """Executes Grail .pym scripts in an isolated process."""
    
    def __init__(self, script_dir: Path):
        self.script_dir = script_dir

    async def execute(self, tool_call: ToolCall, inputs: dict[str, Any]) -> ToolResult:
        script_path = self.script_dir / f"{tool_call.name}.pym"
        if not script_path.exists():
            return ToolResult(
                name=tool_call.name, 
                output=f"Tool script {tool_call.name}.pym not found", 
                call_id=tool_call.call_id, 
                is_error=True
            )

        env = os.environ.copy()
        env["GRAIL_INPUT"] = json.dumps({**inputs, **tool_call.arguments})
        
        proc = await asyncio.create_subprocess_exec(
            "pym", str(script_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env=env
        )
        
        stdout, stderr = await proc.communicate()
        is_error = proc.returncode != 0
        
        try:
            output = json.loads(stdout.decode())
        except json.JSONDecodeError:
            output = stdout.decode().strip() or stderr.decode().strip()
            
        return ToolResult(
            name=tool_call.name,
            output=output,
            call_id=tool_call.call_id,
            is_error=is_error
        )
```

---

## Step 4: The Core Tool Kernel Engine

**File: `src/xgrammar_tools/kernel.py`**
```python
from typing import Any
from openai import AsyncOpenAI
from xgrammar_tools.models import KernelConfig, KernelResult, EventCallback
from xgrammar_tools.plugins.base import ModelPlugin
from xgrammar_tools.backends.base import ToolBackend

class ToolKernel:
    def __init__(self, config: KernelConfig, plugin: ModelPlugin, backend: ToolBackend):
        self.config = config
        self.plugin = plugin
        self.backend = backend
        self._client = AsyncOpenAI(base_url=config.base_url, api_key="dummy")

    async def run(
        self, 
        prompt: str, 
        system: str | None = None,
        tools: list[dict[str, Any]] | None = None,
        max_turns: int = 15,
        observer: EventCallback | None = None
    ) -> KernelResult:
        
        tools = tools or []
        history = self.plugin.build_messages(prompt, system, None)
        turn_count = 0
        final_text = ""
        all_calls = []
        all_results = []

        while turn_count < max_turns:
            turn_count += 1
            if observer:
                await observer.on_model_request({"turn": turn_count, "messages": history})

            # In a real implementation with xgrammar, you would build the grammar state here
            # and pass `extra_body={"structured_outputs": ...}` to vLLM.
            response = await self._client.chat.completions.create(
                model=self.config.model,
                messages=history,
                tools=tools, # type: ignore
                tool_choice=self.plugin.tool_choice(tools),
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )

            msg = response.choices[0].message
            content = msg.content or ""
            final_text += content
            
            if observer:
                await observer.on_model_response({"content": content})

            # Handle explicit tool_calls from API or parse them out of content
            calls = self.plugin.parse_response(content)
            if msg.tool_calls:
                # If using standard OpenAI tools instead of pure grammar strings
                pass 
                
            if not calls:
                break
                
            history.append({"role": "assistant", "content": content})
            all_calls.extend(calls)

            for call in calls:
                if observer:
                    await observer.on_tool_execute({"name": call.name, "args": call.arguments})
                
                # We assume base inputs are injected earlier or passed via the bundle
                result = await self.backend.execute(call, inputs={})
                all_results.append(result)
                
                if observer:
                    await observer.on_tool_result({"name": call.name, "output": result.output})
                
                history.append({
                    "role": "tool",
                    "tool_call_id": result.call_id,
                    "name": result.name,
                    "content": str(result.output)
                })

        return KernelResult(text=final_text, tool_calls=all_calls, tool_results=all_results)
```

---

## Step 5: Directory-Based Asset Bundles

**File: `src/xgrammar_tools/bundles/loader.py`**
```python
import yaml
from pathlib import Path
from pydantic import BaseModel
from xgrammar_tools.plugins.qwen import QwenPlugin
from xgrammar_tools.backends.pym_executor import PymExecutor

class BundleManifest(BaseModel):
    model_id: str
    plugin: str
    system_prompt_file: str = "system.txt"
    lora_adapter: str | None = None
    tools: list[dict] = []

class AgentBundle:
    def __init__(self, path: Path, manifest: BundleManifest):
        self.path = path
        self.manifest = manifest

    def get_plugin(self):
        if self.manifest.plugin == "qwen":
             return QwenPlugin()
        raise ValueError(f"Unknown plugin {self.manifest.plugin}")

    def get_backend(self):
        # Assumes the bundle directory has a 'tools' subfolder containing .pym files
        return PymExecutor(script_dir=self.path / "tools")

    def get_system_prompt(self) -> str:
        with open(self.path / self.manifest.system_prompt_file) as f:
            return f.read()

def load_bundle(directory: str | Path) -> AgentBundle:
    path = Path(directory)
    with open(path / "bundle.yaml") as f:
        data = yaml.safe_load(f)
    manifest = BundleManifest.model_validate(data)
    return AgentBundle(path, manifest)
```

**Testing: Handled via `pytest` simulating a bundle directory creation.**
